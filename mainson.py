# -*- coding: utf-8 -*-

# Form implementation generated from reading ui file 'mainson.ui'
#
# Created by: PyQt5 UI code generator 5.15.7
#
# WARNING: Any manual changes made to this file will be lost when pyuic5 is
# run again.  Do not edit this file unless you know what you are doing.


from PyQt5 import QtCore, QtGui, QtWidgets
import sys
import time
from pathlib import Path
import datetime
import pandas as pd
import pygsheets
import cv2
import torch
import torch.backends.cudnn as cudnn
from numpy import random

from models.experimental import attempt_load
from utils.datasets import LoadStreams, LoadImages
from utils.general import check_img_size, check_imshow, non_max_suppression, apply_classifier, scale_coords, set_logging, increment_path
from utils.torch_utils import select_device, load_classifier, time_synchronized

import core.utils as utils
import copy

import tensorflow as tf
from scipy.signal import butter, filtfilt
import seaborn as sns
import matplotlib.pyplot as plt
import yaml
from tqdm import tqdm
from numba import jit


class Ui_MainWindow(object):
    def setupUi(self, MainWindow):
        MainWindow.setObjectName("MainWindow")
        MainWindow.resize(513, 713)
        MainWindow.setMinimumSize(QtCore.QSize(400, 500))
        self.centralwidget = QtWidgets.QWidget(MainWindow)
        self.centralwidget.setObjectName("centralwidget")
        self.verticalLayoutWidget = QtWidgets.QWidget(self.centralwidget)
        self.verticalLayoutWidget.setGeometry(QtCore.QRect(0, 0, 511, 661))
        self.verticalLayoutWidget.setObjectName("verticalLayoutWidget")
        self.verticalLayout = QtWidgets.QVBoxLayout(self.verticalLayoutWidget)
        self.verticalLayout.setContentsMargins(0, 0, 0, 0)
        self.verticalLayout.setObjectName("verticalLayout")
        self.label = QtWidgets.QLabel(self.verticalLayoutWidget)
        font = QtGui.QFont()
        font.setPointSize(14)
        self.label.setFont(font)
        self.label.setObjectName("label")
        self.verticalLayout.addWidget(self.label, 0, QtCore.Qt.AlignHCenter)
        self.lineEdit = QtWidgets.QLineEdit(self.verticalLayoutWidget)
        self.lineEdit.setMinimumSize(QtCore.QSize(50, 30))
        self.lineEdit.setObjectName("lineEdit")
        self.verticalLayout.addWidget(self.lineEdit, 0, QtCore.Qt.AlignHCenter)
        self.label_3 = QtWidgets.QLabel(self.verticalLayoutWidget)
        font = QtGui.QFont()
        font.setPointSize(14)
        self.label_3.setFont(font)
        self.label_3.setObjectName("label_3")
        self.verticalLayout.addWidget(self.label_3, 0, QtCore.Qt.AlignHCenter)
        self.lineEdit_2 = QtWidgets.QLineEdit(self.verticalLayoutWidget)
        self.lineEdit_2.setMinimumSize(QtCore.QSize(0, 50))
        self.lineEdit_2.setObjectName("lineEdit_2")
        self.verticalLayout.addWidget(self.lineEdit_2)
        self.pushButton = QtWidgets.QPushButton(self.verticalLayoutWidget)
        font = QtGui.QFont()
        font.setPointSize(16)
        self.pushButton.setFont(font)
        self.pushButton.setObjectName("pushButton")
        self.pushButton.clicked.connect(self.detect)        
        self.verticalLayout.addWidget(self.pushButton, 0, QtCore.Qt.AlignHCenter|QtCore.Qt.AlignVCenter)
        self.plainTextEdit = QtWidgets.QPlainTextEdit(self.verticalLayoutWidget)
        self.plainTextEdit.setMinimumSize(QtCore.QSize(500, 350))
        self.plainTextEdit.setLayoutDirection(QtCore.Qt.LeftToRight)
        self.plainTextEdit.setObjectName("plainTextEdit")
        self.plainTextEdit.setReadOnly(True) 
        self.verticalLayout.addWidget(self.plainTextEdit, 0, QtCore.Qt.AlignHCenter|QtCore.Qt.AlignBottom)
        MainWindow.setCentralWidget(self.centralwidget)
        self.menubar = QtWidgets.QMenuBar(MainWindow)
        self.menubar.setGeometry(QtCore.QRect(0, 0, 513, 26))
        self.menubar.setObjectName("menubar")
        MainWindow.setMenuBar(self.menubar)
        self.statusbar = QtWidgets.QStatusBar(MainWindow)
        self.statusbar.setObjectName("statusbar")
        MainWindow.setStatusBar(self.statusbar)

        self.retranslateUi(MainWindow)
        QtCore.QMetaObject.connectSlotsByName(MainWindow)

    def retranslateUi(self, MainWindow):
        _translate = QtCore.QCoreApplication.translate
        MainWindow.setProperty("EYEDIUS: Person Counter", _translate("MainWindow", "MainWindow"))
        self.label.setText(_translate("MainWindow", "Model Name:"))
        self.lineEdit.setText(_translate("MainWindow", "yolov5s.pt"))
        self.label_3.setText(_translate("MainWindow", "Source:"))
        self.lineEdit_2.setText(_translate("MainWindow", "rtsp://admin:eyedius2021@192.168.1.108:554/cam/realmonitor?channel=4&subtype=0&unicast=true&proto=Onvif"))
        self.pushButton.setText(_translate("MainWindow", "Detect"))


    def message(self, s):
        self.plainTextEdit.appendPlainText(s)

    def detect(self):
        self.message("Executing process")

        model_name = self.lineEdit.text()
        source = self.lineEdit_2.text()

        # Insert the rest of your code here, replacing "opt.source" and "opt.weights" with "source" and "model_name" respectively

        gc = pygsheets.authorize(service_file='creds.json')
        # Create empty dataframe
        df = pd.DataFrame()

        startedtime=datetime.datetime.now()
        print("Started at", startedtime)

        counter_dict ={}
        prev_dict = {}
        #source, weights, view_img, save_txt, imgsz = opt.source, opt.weights, opt.view_img, opt.save_txt, opt.img_size
        source, weights, view_img, save_txt, imgsz = source, model_name, True, True, 640
        save_img = not False and not source.endswith('.txt')  # save inference images
        webcam = source.isnumeric() or source.endswith('.txt') or source.lower().startswith(
            ('rtsp://', 'rtmp://', 'http://', 'https://'))

        # Directories
        save_dir = Path(increment_path(Path('runs/detect') / 'exp', exist_ok=False))  # increment run
        (save_dir / 'labels' if save_txt else save_dir).mkdir(parents=True, exist_ok=True)  # make dir

        # Initialize
        set_logging()
        device = select_device('cpu')
        half = device.type != 'cpu'  # half precision only supported on CUDA

        # Load model
        model = attempt_load(weights, map_location=device)  # load FP32 model
        stride = int(model.stride.max())  # model stride
        imgsz = check_img_size(imgsz, s=stride)  # check img_size
        if half:
            model.half()  # to FP16

        # Second-stage classifier
        classify = False
        if classify:
            modelc = load_classifier(name='resnet101', n=2)  # initialize
            modelc.load_state_dict(torch.load('weights/resnet101.pt', map_location=device)['model']).to(device).eval()

        # Create ROI 
        vs = cv2.VideoCapture(source)
        print(imgsz)
        frame = vs.read()
        frame = cv2.resize(frame[1], (imgsz, imgsz))
        roi = True
        if roi is not False:
            roi = cv2.selectROI(frame,showCrosshair=False)
            cv2.destroyWindow('ROI selector')  
            print(roi)
            self.message("ROI selected")  
        else:
            print("[INFO] roi not requested")
        vs.release()        

        # Set Dataloader
        vid_path, vid_writer = None, None
        if webcam:
            view_img = check_imshow()
            cudnn.benchmark = True  # set True to speed up constant image size inference
            dataset = LoadStreams(source, img_size=imgsz, stride=stride, roi=roi)
        else:
            dataset = LoadImages(source, img_size=imgsz, stride=stride, roi=roi)

        # Get names and colors
        names = model.module.names if hasattr(model, 'module') else model.names
        colors = [[random.randint(0, 255) for _ in range(3)] for _ in names]

        # Run inference
        if device.type != 'cpu':
            model(torch.zeros(1, 3, imgsz, imgsz).to(device).type_as(next(model.parameters())))  # run once
        startPushTime = time.time()
        for path, img, im0s, vid_cap in dataset:
            img = torch.from_numpy(img).to(device)
            img = img.half() if half else img.float()  # uint8 to fp16/32
            img /= 255.0  # 0 - 255 to 0.0 - 1.0
            if img.ndimension() == 3:
                img = img.unsqueeze(0)

            prev_time = time.time()

            # Inference
            t1 = time_synchronized()
            pred = model(img, augment=False)[0]
            # Apply NMS
            pred = non_max_suppression(pred, 0.18, 0.2, classes=0, agnostic=False)
            t2 = time_synchronized()
           
            # Apply Classifier
            if classify:
                pred = apply_classifier(pred, modelc, img, im0s)

            # Process detections
            for i, det in enumerate(pred):  # detections per image
                # print(type(det))
                if webcam:  # batch_size >= 1
                    p, s, im0, frame = path[i], '%g: ' % i, im0s[i].copy(), dataset.count
                else:
                    p, s, im0, frame = path, '', im0s, getattr(dataset, 'frame', 0)


                p = Path(p)  # to Path
                save_path = str(save_dir / p.name)  # img.jpg
                txt_path = str(save_dir / 'labels' / p.stem) + ('' if dataset.mode == 'image' else f'_{frame}')  # img.txt
                s += '%gx%g ' % img.shape[2:]  # print string
                gn = torch.tensor(im0.shape)[[1, 0, 1, 0]]  # normalization gain whwh
                if len(det):
                    # Rescale boxes from img_size to im0 size
                    det[:, :4] = scale_coords(img.shape[2:], det[:, :4], im0.shape).round()
                    # print(det)
                    # print(type(det.tolist()[0]))
                    frame = cv2.cvtColor(im0, cv2.COLOR_BGR2RGB)
                    det = det.tolist()
                    # list_of_numpy = []
                    # for li in det.cpu():
                    #     list_of_numpy.append(np.asarray(li, dtype=np.float32))
                    # print(list_of_numpy[0].shape, list_of_numpy[0])
                    curr_time = time.time()
                    exec_time_1 = curr_time - prev_time
                    fps = 1/exec_time_1
                    image, counter_dict = utils.video_draw_bbox(frame, det, fps)
                    # FPS LOG
                    curr_time_2 = time.time()
                    exec_time_2 = curr_time_2 - prev_time

                    #self.message(str(startPushTime))   
                    if time.time() - startPushTime> 40:
                        dictToPush = {"pushTime": time.time()} 
                        startPushTime = time.time()
                        #self.message(str(startPushTime))   

                        try: 
                            dictToPush["person"] = {"up": counter_dict["person"]["up"] - prev_dict["person"]["up"], "down": counter_dict["person"]["down"] - prev_dict["person"]["down"], "left": counter_dict["person"]["left"] - prev_dict["person"]["left"], "right": counter_dict["person"]["right"] - prev_dict["person"]["right"]}
                            #time.sleep(29)
                        except:
                            if "person" in counter_dict.keys():
                                dictToPush["person"] = counter_dict["person"]
                        # try: 
                        #     dictToPush["car"] = {"up": counter_dict["car"]["up"] - prev_dict["car"]["up"], "down": counter_dict["car"]["down"] - prev_dict["car"]["down"], "left": counter_dict["car"]["left"] - prev_dict["car"]["left"], "right": counter_dict["car"]["right"] - prev_dict["car"]["right"]}
                        # except:
                        #     if "car" in counter_dict.keys():
                        #         dictToPush["car"] = counter_dict["car"]
                        # try: 
                        #     dictToPush["motorcycle"] = {"up": counter_dict["motorcycle"]["up"] - prev_dict["motorcycle"]["down"], "down": counter_dict["motorcycle"]["down"] - prev_dict["motorcycle"]["up"], "left": counter_dict["motorcycle"]["left"] - prev_dict["motorcycle"]["left"], "right": counter_dict["motorcycle"]["right"] - prev_dict["motorcycle"]["right"]}
                        # except:
                        #     if "motorcycle" in counter_dict.keys():
                        #         dictToPush["motorcycle"] = counter_dict["motorcycle"]
                        # try: 
                        #     dictToPush["bus"] = {"up": counter_dict["bus"]["up"] - prev_dict["bus"]["up"], "down": counter_dict["bus"]["down"] - prev_dict["bus"]["down"], "left": counter_dict["bus"]["left"] - prev_dict["bus"]["left"], "right": counter_dict["bus"]["right"] - prev_dict["bus"]["right"]}
                        # except:
                        #     if "bus" in counter_dict.keys():
                        #         dictToPush["bus"] = counter_dict["bus"]
                        # try: 
                        #     dictToPush["truck"] = {"up": counter_dict["truck"]["up"] - prev_dict["truck"]["up"], "down": counter_dict["truck"]["down"] - prev_dict["truck"]["down"], "left": counter_dict["truck"]["left"] - prev_dict["truck"]["left"], "right": counter_dict["truck"]["right"] - prev_dict["truck"]["right"]}
                        # except:
                        #     if "truck" in counter_dict.keys():
                        #         dictToPush["truck"] = counter_dict["truck"]

                        # result=db.test.insert_one(dictToPush)
                        #print(result)
                        #print(dictToPush)
                        #print(dictToPush["person"])
                        self.message(str(dictToPush["person"]))

                        #print(type(dictToPush["person"]))
                        totalP = int(dictToPush["person"]['up'])+int(dictToPush["person"]['down'])
                        data = {
                        "Total Person": [totalP],
                        "Total Up": dictToPush["person"]['up'],
                        "Total Down": dictToPush["person"]['down'],
                        "Starting Time": [startedtime.strftime("%c")],
                        "Finishing Time": [time.strftime("%c")]  
                        }
                        #load data into a DataFrame object:
                        df = pd.DataFrame(data)
                        #open the google spreadsheet (where 'PersonCounter' is the name of my sheet)
                        sh = gc.open('PersonCounter')
                        #select the first sheet 
                        wks = sh[0]
                        values = df.values.tolist()
                        #update the first sheet with df 
                        wks.append_table(values, start='A1', end=None, dimension='ROWS', overwrite=False)
                        print("logged into Google Spread")    
                        self.message("logged into Google Spread")                

                        prev_dict = copy.deepcopy(counter_dict)

                #     # Print results
                #     for c in det[:, -1].unique():
                #         n = (det[:, -1] == c).sum()  # detections per class
                #         s += f"{n} {names[int(c)]}{'s' * (n > 1)}, "  # add to string

                #     # Write results
                #     for *xyxy, conf, cls in reversed(det):
                #         if save_txt:  # Write to file
                #             xywh = (xyxy2xywh(torch.tensor(xyxy).view(1, 4)) / gn).view(-1).tolist()  # normalized xywh
                #             line = (cls, *xywh, conf) if False else (cls, *xywh)  # label format
                #             with open(txt_path + '.txt', 'a') as f:
                #                 f.write(('%g ' * len(line)).rstrip() % line + '\n')

                #         if save_img or view_img:  # Add bbox to image
                #             label = f'{names[int(cls)]} {conf:.2f}'
                #             plot_one_box(xyxy, im0, label=label, color=colors[int(cls)], line_thickness=3)

                # # Print time (inference + NMS)
                # print(f'{s}Done. ({t2 - t1:.3f}s)')

                # Stream results
            # print(image)
            # result = cv2.cvtColor(image, cv2.COLOR_RGB2BGR) 
            view_img = True
            if view_img:
                result = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)
                cv2.imshow("frame", result)
                cv2.waitKey(1)  # 1 millisecond
            
        #         #Save results (image with detections)
        #         save_img=True
        #         if save_img:
        #             if dataset.mode == 'image':
        #                 cv2.imwrite(save_path, im0)
        #             else:  # 'video' or 'stream'
        #                 if vid_path != save_path:  # new video
        #                     vid_path = save_path
        #                     if isinstance(vid_writer, cv2.VideoWriter):
        #                         vid_writer.release()  # release previous video writer
        #                     if vid_cap:  # video
        #                         fps = vid_cap.get(cv2.CAP_PROP_FPS)
        #                         w = int(vid_cap.get(cv2.CAP_PROP_FRAME_WIDTH))
        #                         h = int(vid_cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
        #                     else:  # stream
        #                         fps, w, h = 30, im0.shape[1], im0.shape[0]
        #                         save_path += '.mp4'
        #                     vid_writer = cv2.VideoWriter(save_path, cv2.VideoWriter_fourcc(*'mp4v'), 30, (600, 600))
        #                 vid_writer.write(im0)

        # if save_txt or save_img:
        #     s = f"\n{len(list(save_dir.glob('labels/*.txt')))} labels saved to {save_dir / 'labels'}" if save_txt else ''
        #     print(f"Results saved to {save_dir}{s}")     
        #print(f'Done. ({time.time() - t0:.3f}s)')


if __name__ == "__main__":
    import sys
    app = QtWidgets.QApplication(sys.argv)
    MainWindow = QtWidgets.QMainWindow()
    ui = Ui_MainWindow()
    ui.setupUi(MainWindow)
    MainWindow.show()
    sys.exit(app.exec_())
